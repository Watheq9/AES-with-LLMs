
# Automated Essay Scoring with Large Language Models

This repository contains the data resulted from our paper titled "Can Large Language Models Automatically Score Proficiency of Written Essays?", that is published in LREC-COLING 2024 conference.

The data represents the responses of two LLMs (ChatGPT and LLama2) on the automated essay scoring task over the ASAP dataset.

There are just two folders in this repo, namely ChatGPT and LLama, named after the experimented models.  In each folder, there are eight subfolders; each subfolder corresponds to the task number in ASAP, and contains the responses of the experimented prompts (Prompt A, B, C & D).


## Citation

If you use this data, please cite our paper:

```
@inproceedings{mansour2024llms,
  title     = {Can Large Language Models Automatically Score Proficiency of Written Essays?},
  author    = {Mansour, Watheq and Albatarni, Salam and Eltanbouly, Sohaila and Elsayed, Tamer},
  booktitle = {Proceedings of The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  address   = {Torino, Italy},
  year      = {2024}
}
```
